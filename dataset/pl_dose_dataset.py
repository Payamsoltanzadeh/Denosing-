# ============================================================================
# MODIFICATIONS LOG - GitHub Copilot
# ============================================================================
# Date: 2025-11-30
# Changes:
#   1. Modified dataset indexing to handle both 'output' subfolder and direct
#      folder structure (for Mini_Dataset compatibility)
#   2. Added try/except for energy parsing to skip invalid folder names
#   3. Added check for batch_path being a directory
#   4. Modified setup() to handle small datasets (<3 samples)
#   5. Added print statements for dataset size debugging
#   6. Changed test_set subset to use min(200, actual_size)
# Why: Mini_Dataset has different folder structure and very few samples
# ----------------------------------------------------------------------------
# Date: 2025-12-02
# Changes:
#   7. Added add_poisson_noise() function for simulating Low-Photon MC
#   8. Added num_photons and add_noise parameters to ConditionalDoseDataset
#   9. Modified __getitem__ to create LP dose from HP dose using Poisson noise
#   10. Added lp_dose to condition dict for Diff-MC pipeline
# Why: Adapt to Diff-MC paper (Professor Hesser Session 3):
#      "Take that as mean value and then sample... and then you generate 
#       Poisson images, because this is easy to go."
#      Data analysis showed: input_cubes = CT (-1000 to 1207)
#                           output_cubes = Dose (0.000001 to 0.017557)
# ----------------------------------------------------------------------------
# Date: 2025-12-03
# Changes:
#   11. Updated dataset to load pre-generated LP dose from lp_cubes/ folder
#   12. Added use_pregenerated_lp option to switch between:
#       - True: Load LP from lp_cubes/ (pre-generated by generate_lp_dose.py)
#       - False: Generate LP on-the-fly from HP using Poisson noise
#   13. Restructured __getitem__ to clearly separate CT, LP, HP
#   14. Added detailed comments explaining the Diff-MC pipeline
# Why: User requested file-based LP dose storage for reproducibility
#      and to match Diff-MC paper methodology.
# 
# Data Flow (Diff-MC style):
#   input_cubes/   → CT volume    (anatomy, HU: -1000 to 1200)
#   lp_cubes/      → LP dose      (noisy input, pre-generated)
#   output_cubes/  → HP dose      (clean target, ground truth)
#
# Model learns: f(CT, LP, energy) → HP  (conditional denoising)
# ============================================================================

import os
import numpy as np
import torch
from torch.utils.data import Dataset
from tqdm import tqdm
import glob
import pytorch_lightning as pl
from torch.utils.data import random_split, DataLoader
import torch.nn.functional as F


# ============================================================================
# Date: 2025-12-02
# Function: add_poisson_noise
# Why: Professor Hesser (Session 3) requested Poisson noise to simulate 
#      Low-Photon Monte Carlo. This is multiplicative noise, not additive.
#      "Take that as mean value and then sample... just say, okay, 
#       I have now so and so many photons... and then you generate 
#       Poisson images"
# Note (2025-12-03): This function is now used as fallback when 
#      use_pregenerated_lp=False. For reproducibility, prefer using
#      pre-generated LP dose from lp_cubes/ folder.
# ============================================================================
def add_poisson_noise(clean_dose: np.ndarray, num_photons: float = 1e6) -> np.ndarray:
    """
    Simulate Low-Photon Monte Carlo by adding Poisson noise to clean dose.
    
    Args:
        clean_dose: High-Photon MC dose (ground truth, 10^10 photons)
        num_photons: Number of photons to simulate
                     - 1e10 = no noise (original)
                     - 1e8  = low noise (100x speedup)
                     - 1e6  = medium noise (10000x speedup)
                     - 1e4  = high noise
    
    Returns:
        noisy_dose: Low-Photon MC dose (simulated)
    """
    # Ensure non-negative (dose cannot be negative)
    clean_dose = np.maximum(clean_dose, 0)
    
    # Scale to photon counts
    scaled = clean_dose * num_photons
    
    # Sample from Poisson distribution (this is the Monte Carlo simulation)
    noisy = np.random.poisson(scaled).astype(np.float32)
    
    # Scale back to original range
    noisy_dose = noisy / num_photons
    
    return noisy_dose


class ConditionalDoseDataset(Dataset):
    # ========================================================================
    # Date: 2025-12-03
    # Changes: Added use_pregenerated_lp parameter
    # Why: Support both pre-generated LP (reproducible) and on-the-fly LP
    # ========================================================================
    def __init__(self, root_dir, normalize=True, target_dim=None, 
                 num_photons=1e5, add_noise=True, use_pregenerated_lp=True,
                 lp_folder="lp_cubes"):
        """
        Diff-MC style dataset for Monte Carlo dose denoising.
        
        Data folders:
            input_cubes/   → CT volume (anatomy, HU values)
            output_cubes/  → HP dose (High-Photon, clean, target)
            {lp_folder}/   → LP dose (Low-Photon, noisy, input) [if use_pregenerated_lp=True]
        
        Args:
            root_dir: Path to the dataset root.
            normalize: Whether to normalize each volume individually.
            target_dim: Target spatial dimension (e.g., 128 → 128³)
            num_photons: Number of photons for on-the-fly Poisson noise 
                         (only used if use_pregenerated_lp=False)
            add_noise: Whether to add noise (False for clean LP = HP)
            use_pregenerated_lp: If True, load LP from lp_folder.
                                 If False, generate LP on-the-fly from HP.
            lp_folder: Name of the folder containing LP dose files (default: "lp_cubes")
        """
        super().__init__()
        self.data = []
        self.normalize = normalize
        self.target_dim = target_dim
        self.num_photons = num_photons
        self.add_noise = add_noise
        self.use_pregenerated_lp = use_pregenerated_lp
        self.lp_folder = lp_folder

        beam_folders = sorted(os.listdir(root_dir))
        for beam_folder in tqdm(beam_folders, desc="Indexing dataset"):
            # Check for 'output' folder, if not present, assume direct structure
            beam_path_output = os.path.join(root_dir, beam_folder, "output")
            if os.path.isdir(beam_path_output):
                 beam_path = beam_path_output
            else:
                 beam_path = os.path.join(root_dir, beam_folder)
            
            if not os.path.isdir(beam_path):
                continue

            try:
                energy = float(beam_folder.replace("_", "."))
            except ValueError:
                continue

            for batch_id in os.listdir(beam_path):
                batch_path = os.path.join(beam_path, batch_id)
                if not os.path.isdir(batch_path):
                    continue
                
                # ============================================================
                # Date: 2025-12-03 - Define paths for CT, HP, and LP
                # ============================================================
                input_path = os.path.join(batch_path, "input_cubes")   # CT
                output_path = os.path.join(batch_path, "output_cubes") # HP dose
                lp_path = os.path.join(batch_path, self.lp_folder)     # LP dose (Dynamic folder)

                input_files = sorted(glob.glob(os.path.join(input_path, "*.npy")))
                for file in input_files:
                    filename = os.path.basename(file)
                    ct_file = os.path.join(input_path, filename)       # CT
                    hp_file = os.path.join(output_path, filename)      # HP dose
                    lp_file = os.path.join(lp_path, filename)          # LP dose
                    
                    # Check if HP dose exists (required)
                    if not os.path.exists(hp_file):
                        continue
                    
                    # Check if LP dose exists (if using pre-generated)
                    if self.use_pregenerated_lp and not os.path.exists(lp_file):
                        # LP file doesn't exist, skip or warn
                        print(f"Warning: LP dose not found: {lp_file}")
                        print("Run generate_lp_dose.py first, or set use_pregenerated_lp=False")
                        lp_file = None
                    
                    # Store paths: (CT, HP, LP, energy)
                    self.data.append((ct_file, hp_file, lp_file, energy))

    def __len__(self):
        return len(self.data)

    def reshape_tensor(self, tensor):
        """Resizes a 4D tensor (1, D, H, W) to (1, target_dim, target_dim, target_dim)."""
        if self.target_dim is None:
            return tensor
        return F.interpolate(tensor.unsqueeze(0), size=(self.target_dim,) * 3, mode='trilinear', align_corners=False).squeeze(0)

    # ========================================================================
    # Date: 2025-12-03
    # Changes: Complete rewrite of __getitem__ for Diff-MC pipeline
    # 
    # Data Loading:
    #   - CT volume:  from input_cubes/  (anatomy, HU: -1000 to 1200)
    #   - HP dose:    from output_cubes/ (clean, ground truth)
    #   - LP dose:    from lp_cubes/ or generated on-the-fly
    #
    # Model Input/Output:
    #   - Input (condition): CT + LP dose + beam energy
    #   - Output (target):   HP dose
    #
    # Channel Organization:
    #   - Each volume is [1, D, H, W] (single channel, 3D)
    #   - Condition dict contains separate tensors for CT and LP
    #   - Model can concatenate [CT, LP] as 2-channel input if needed
    # ========================================================================
    def __getitem__(self, idx):
        ct_file, hp_file, lp_file, energy = self.data[idx]
        
        # ====================================================================
        # Load CT volume (anatomy)
        # Source: input_cubes/
        # Range: -1000 (air) to ~1200 (bone) HU
        # Purpose: Anatomical context for dose prediction
        # ====================================================================
        ct_vol = np.load(ct_file).astype(np.float32)
        
        # ====================================================================
        # Load HP dose (High-Photon = clean = target)
        # Source: output_cubes/
        # Range: ~1e-6 to ~1e-2 (dose values)
        # Purpose: Ground truth for training (what model should predict)
        # ====================================================================
        hp_dose = np.load(hp_file).astype(np.float32)
        
        # ====================================================================
        # Load or Generate LP dose (Low-Photon = noisy = input)
        # Source: lp_cubes/ (pre-generated) or on-the-fly from HP
        # Range: Similar to HP but with Poisson noise
        # Purpose: Noisy input for denoising task
        # ====================================================================
        if self.use_pregenerated_lp and lp_file is not None:
            # Load pre-generated LP dose (reproducible)
            lp_dose = np.load(lp_file).astype(np.float32)
        elif self.add_noise:
            # Generate LP dose on-the-fly (random each epoch)
            lp_dose = add_poisson_noise(hp_dose, self.num_photons)
        else:
            # No noise (for testing/ablation)
            lp_dose = hp_dose.copy()
        
        # ====================================================================
        # Normalize each volume (z-score normalization)
        # ====================================================================
        if self.normalize:
            ct_vol = (ct_vol - ct_vol.mean()) / (ct_vol.std() + 1e-5)
            lp_dose = (lp_dose - lp_dose.mean()) / (lp_dose.std() + 1e-5)
            hp_dose = (hp_dose - hp_dose.mean()) / (hp_dose.std() + 1e-5)

        # ====================================================================
        # Convert to PyTorch tensors
        # Shape: [1, D, H, W] (1 channel, 3D volume)
        # ====================================================================
        ct_tensor = torch.from_numpy(ct_vol).unsqueeze(0)      # [1, D, H, W]
        lp_tensor = torch.from_numpy(lp_dose).unsqueeze(0)     # [1, D, H, W]
        hp_tensor = torch.from_numpy(hp_dose).unsqueeze(0)     # [1, D, H, W]

        # ====================================================================
        # Reshape to target dimension if specified
        # ====================================================================
        ct_tensor = self.reshape_tensor(ct_tensor)
        lp_tensor = self.reshape_tensor(lp_tensor)
        hp_tensor = self.reshape_tensor(hp_tensor)

        # ====================================================================
        # Construct condition dictionary
        # 
        # The model receives this dict as conditioning information:
        #   - "ct": CT volume for anatomical context
        #   - "lp_dose": Low-Photon dose (noisy input to be denoised)
        #   - "energy": Beam energy in keV
        #
        # Model architectures can use this in different ways:
        #   1. Concatenate [CT, LP] as 2-channel input
        #   2. Use CT as cross-attention context
        #   3. Use energy as class embedding
        # ====================================================================
        condition = {
            "ct": ct_tensor,                                    # [1, D, H, W] - Anatomy
            "lp_dose": lp_tensor,                               # [1, D, H, W] - Noisy input
            "energy": torch.tensor([energy], dtype=torch.float32)  # [1] - Beam energy
        }

        # ====================================================================
        # Return: (target, condition)
        #   - target = HP dose (what model should predict)
        #   - condition = dict with CT, LP, energy (model input)
        # ====================================================================
        return hp_tensor, condition


class DoseDataModule(pl.LightningDataModule):
    def __init__(self, root_dir, batch_size=4, num_workers=4, split=(0.8, 0.1, 0.1), target_dim=None):
        super().__init__()
        assert sum(split) == 1.0, "Splits must sum to 1."
        self.root_dir = root_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.split = split
        self.target_dim = target_dim

    def setup(self, stage=None):
        full_dataset = ConditionalDoseDataset(self.root_dir, target_dim=self.target_dim)

        total = len(full_dataset)
        print(f"Total dataset size: {total} samples")
        
        # Handle small datasets by ensuring at least 1 sample in each split
        if total < 3:
            # For very small datasets, use same data for all splits
            train_len = total
            val_len = 0
            test_len = 0
            self.train_set = full_dataset
            self.val_set = full_dataset
            self.test_set = full_dataset
        else:
            train_len = max(1, int(self.split[0] * total))
            val_len = max(1, int(self.split[1] * total))
            test_len = total - train_len - val_len
            if test_len < 1:
                test_len = 1
                train_len = total - val_len - test_len
            
            self.train_set, self.val_set, self.test_set = random_split(
                full_dataset, [train_len, val_len, test_len]
            )

        print(f"Train: {len(self.train_set)}, Val: {len(self.val_set)}, Test: {len(self.test_set)}")

        # Limit test dataset to min(200, actual size)
        test_size = min(200, len(self.test_set))
        if test_size > 0:
            self.test_set = torch.utils.data.Subset(self.test_set, list(range(test_size)))

    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)

    def val_dataloader(self):
        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

    def test_dataloader(self):
        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)


if __name__ == '__main__':
    # each energy has 10 batches
    # each batch has 10 patients (100 total patients)
    # 200 patients cubes per patient
    # = 160 000 sample cubes (100x100x100) in the entire dataset
    import random

    # Point this to your dataset root
    root_dir = "/hdd/Josch_Data/simulations"

    # Instantiate the dataset
    dataset = ConditionalDoseDataset(root_dir=root_dir, normalize=True)

    print(f"✅ Loaded dataset with {len(dataset)} samples")

    # Pick a random sample
    index = random.randint(0, len(dataset) - 1)
    input_tensor, condition, target_tensor = dataset[index]

    print("\n--- Sample Inspection ---")
    print(f"Index: {index}")
    print(f"Input CT shape:        {input_tensor.shape} (should be [1, D, H, W])")
    print(f"Target Dose shape:     {target_tensor.shape} (should be [1, D, H, W])")
    print(f"Condition 'ct' shape:  {condition['ct'].shape} (copy of input)")
    print(f"Condition 'energy':    {condition['energy'].item()} keV")

    assert input_tensor.shape == target_tensor.shape, "❌ Input and target shapes do not match!"
    assert condition["ct"].shape == input_tensor.shape, "❌ Condition CT shape mismatch!"
    assert isinstance(condition["energy"], torch.Tensor), "❌ Energy is not a tensor!"

    print("✅ All checks passed.")